{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import bibtexparser\n",
    "\n",
    "pd.set_option('mode.chained_assignment','warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The harvest() function utilizes arxiv.org's API to pull metadata for the specified data range.\n",
    "# code mostly borrowed from http://betatim.github.io/posts/analysing-the-arxiv/\n",
    "\n",
    "OAI = \"{http://www.openarchives.org/OAI/2.0/}\"\n",
    "ARXIV = \"{http://arxiv.org/OAI/arXiv/}\"\n",
    "\n",
    "def harvest(startyear, endyear):\n",
    "    ''' Uses arXiv's API to pull in all papers from between Jan 1, startyear to Jan 1, endyear''' \n",
    "    df = pd.DataFrame(columns=(\"title\", \"abstract\", \"categories\", \"created\", \"id\", \"doi\"))\n",
    "    base_url = \"http://export.arxiv.org/oai2?verb=ListRecords&\"\n",
    "    url = (base_url +\n",
    "           \"from=\" + str(startyear) + \"-01-01&until=\" + str(endyear) + \"-01-01&\" +\n",
    "           \"metadataPrefix=arXiv&set=math\")\n",
    "    \n",
    "    while True:\n",
    "        print (\"fetching\", url)\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            \n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 503:\n",
    "                to = int(e.hdrs.get(\"retry-after\", 30))\n",
    "                print (\"Got 503. Retrying after {0:d} seconds.\".format(to))\n",
    "\n",
    "                time.sleep(to)\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "        xml = response.read()\n",
    "\n",
    "        root = ET.fromstring(xml)\n",
    "\n",
    "        for record in root.find(OAI+'ListRecords').findall(OAI+\"record\"):\n",
    "            try:\n",
    "                arxiv_id = record.find(OAI+'header').find(OAI+'identifier')\n",
    "                meta = record.find(OAI+'metadata')\n",
    "                info = meta.find(ARXIV+\"arXiv\")\n",
    "                created = info.find(ARXIV+\"created\").text\n",
    "                created = datetime.datetime.strptime(created, \"%Y-%m-%d\")\n",
    "                categories = info.find(ARXIV+\"categories\").text\n",
    "\n",
    "                # if there is more than one DOI use the first one\n",
    "                # often the second one (if it exists at all) refers\n",
    "                # to an eratum or similar\n",
    "                doi = info.find(ARXIV+\"doi\")\n",
    "                if doi is not None:\n",
    "                    doi = doi.text.split()[0]\n",
    "\n",
    "                contents = {'title': info.find(ARXIV+\"title\").text,\n",
    "                            'id': info.find(ARXIV+\"id\").text,#arxiv_id.text[4:],\n",
    "                            'abstract': info.find(ARXIV+\"abstract\").text.strip(),\n",
    "                            'created': created,\n",
    "                            'categories': categories.split(),\n",
    "                            'doi': doi,\n",
    "                            }\n",
    "\n",
    "                df = df.append(contents, ignore_index=True)\n",
    "            except: pass\n",
    "\n",
    "        # The list of articles returned by the API comes in chunks of\n",
    "        # 1000 articles. The presence of a resumptionToken tells us that\n",
    "        # there is more to be fetched.\n",
    "        token = root.find(OAI+'ListRecords').find(OAI+\"resumptionToken\")\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            url = base_url + \"resumptionToken=%s\"%(token.text)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvest all papers from 2007 until 2018\n",
    "# Export each to csv file\n",
    "\n",
    "for year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    df = harvest(year, year+1)\n",
    "    df.to_csv('../data/arxiv_math_' + str(year) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the above dataframes into one and export to csv\n",
    "\n",
    "arxiv_math = pd.DataFrame()\n",
    "for year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    df = pd.read_csv('../data/arxiv_math_' + str(year) + '.csv')\n",
    "    arxiv_math = arxiv_math.append(df, ignore_index = True)\n",
    "    \n",
    "arxiv_math.to_csv('../data/arxiv_math', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
